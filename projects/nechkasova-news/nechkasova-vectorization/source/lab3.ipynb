{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../nechkasova-tokenizer/assets/annotated-corpus/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, encoding=\"ISO-8859-1\") as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    lines = read_data(file_path)\n",
    "    tokens = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                token, stem, lemma = line.strip().split('\\t')\n",
    "            except:\n",
    "                continue\n",
    "            cleaned_token = clean_text(token)\n",
    "            if cleaned_token and cleaned_token not in stop_words:\n",
    "                tokens.append(cleaned_token)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    all_tokens = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Process file {file_path}\")\n",
    "            tokens = process_file(file_path)\n",
    "            all_tokens.append(tokens)\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = process_directory(dataset_path)\n",
    "# print(len(tokens))\n",
    "\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(content):\n",
    "    sentence_endings_pattern = re.compile(r'(?<!\\w\\.\\w.)(?<!\\w\\. \\w.)(?<![A-Z][a-z]\\.)(?<!\\s\\.\\s)(?<=\\.|\\?|\\!)\\s(?![A-Z][A-Za-z]\\.)(?!\\w\\. \\w.)|(?<![,])\\n(?![a-zA-Z0-9])')\n",
    "    sentences = sentence_endings_pattern.split(content)\n",
    "\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    sentences_included_key_value = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        key_value_pattern = re.compile(r'[A-Z](\\w+-)*\\w+(\\s\\w+)*:[ ]*([\\W\\w]+[ ,])+(?!,\\n)')\n",
    "        if key_value_pattern.match(sentence):\n",
    "            add_sentence = re.split(r'(?<!,)\\n', sentence)\n",
    "            sentences_included_key_value.extend(add_sentence)\n",
    "        else:\n",
    "            sentences_included_key_value.append(sentence)\n",
    "    # sentences = [sentence.replace('\\n', ' ') for sentence in sentences]\n",
    "    return sentences_included_key_value\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    pattern = r'\\+?\\d[\\d\\-\\(\\)\\s]{7,}\\d' \\\n",
    "              r'|\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b' \\\n",
    "              r'|\\b(?:Mr|Ms|Mrs|Dr|Prof|St)\\.\\s[A-Z][a-z]+' \\\n",
    "              r'|\\b\\d{1,2}:\\d{2}\\s?(?:[AaPp]\\.?[Mm]\\.?)\\b' \\\n",
    "              r'|\\w+|[^\\w\\s]'\n",
    "    tokens = re.findall(pattern, sentence)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_text(text, model):\n",
    "    sentences = get_sentences(text)\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        cleaned_text = clean_text(sentence)\n",
    "        tokens = tokenize_sentence(cleaned_text)\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "        word_vectors = [model.wv[word] if word in model.wv else np.zeros(100) for word in tokens]\n",
    "        sentence_vector = np.mean(word_vectors, axis=0)\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "        \n",
    "    text_vector = np.mean(sentence_vectors, axis=0)\n",
    "    \n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"programs applications developing. Sentence\"\n",
    "file_tsv_content = {}\n",
    "\n",
    "vector = vectorize_text(text, model)\n",
    "print(len(vector))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "programs = applications, developing (programming)\n",
    "interface, buggy, functions, UNIX, code, signals, Internet\n",
    "people, Canada, sport, tatoo\n",
    "\n",
    "\n",
    "email = phone, FAX\n",
    "address, message, contact\n",
    "team, games, baseball\n",
    "\n",
    "lecture = seminar, courses, \n",
    "education, laboratories, conference\n",
    "situation, comments, season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    return 1 - cosine(vector_a, vector_b)\n",
    "\n",
    "def calculate_distances(word, similar_words, domain_words, different_words, model, cosine_similarity_function):\n",
    "    word_vector = model.wv[word]\n",
    "    distances = []\n",
    "    \n",
    "    for group_name, words in [('Похожие слова', similar_words), \n",
    "                              ('Слова из той же области', domain_words), \n",
    "                              ('Совершенно другие слова', different_words)]:\n",
    "        for w in words:\n",
    "            if w in model.wv:\n",
    "                distance = cosine_similarity_function(word_vector, model.wv[w])\n",
    "                distances.append((group_name, w, distance))\n",
    "    \n",
    "    return sorted(distances, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = [\"applications\", \"developing\"]\n",
    "domain_words = [\"interface\", \"buggy\", \"functions\", \"UNIX\", \"code\", \"signals\", \"Internet\"]\n",
    "different_words = [\"people\", \"Canada\", \"sport\", \"tatoo\"]\n",
    "\n",
    "distances = calculate_distances('programs', similar_words, domain_words, different_words, model, cosine_similarity)\n",
    "\n",
    "for group, word, distance in distances:\n",
    "    print(f\"{group}: {word} - Косинусное сходство: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = [\"address\", \"FAX\"]\n",
    "domain_words = [\"phone\", \"message\", \"contact\"]\n",
    "different_words = [\"team\", \"games\", \"baseball\"]\n",
    "\n",
    "distances = calculate_distances('email', similar_words, domain_words, different_words, model, cosine_similarity)\n",
    "\n",
    "for group, word, distance in distances:\n",
    "    print(f\"{group}: {word} - Косинусное сходство: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = [\"seminar\", \"courses\"]\n",
    "domain_words = [\"education\", \"laboratories\", \"conference\"]\n",
    "different_words = [\"situation\", \"comments\", \"season\"]\n",
    "\n",
    "distances = calculate_distances('lecture', similar_words, domain_words, different_words, model, cosine_similarity)\n",
    "\n",
    "for group, word, distance in distances:\n",
    "    print(f\"{group}: {word} - Косинусное сходство: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_manual(vector_a, vector_b):\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = np.linalg.norm(vector_a)\n",
    "    norm_b = np.linalg.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = [\"seminar\", \"courses\"]\n",
    "domain_words = [\"education\", \"laboratories\", \"conference\"]\n",
    "different_words = [\"situation\", \"comments\", \"season\"]\n",
    "\n",
    "distances = calculate_distances('lecture', similar_words, domain_words, different_words, model, cosine_similarity_manual)\n",
    "\n",
    "for group, word, distance in distances:\n",
    "    print(f\"{group}: {word} - Косинусное сходство: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_content(file_path):\n",
    "    filename = file_path.split('/')[-1]\n",
    "    try:\n",
    "        with open(file_path, encoding=\"ISO-8859-1\") as file:\n",
    "            content = file.read()\n",
    "            return content, filename\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't read file {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "def vectorize_file(file_path):\n",
    "    content, filename = read_content(file_path)\n",
    "    vector = vectorize_text(content, model)\n",
    "    \n",
    "    vector_str = \"\\t\".join([str(component) for component in vector])\n",
    "    result_line = f\"{filename}\\t{vector_str}\"\n",
    "    \n",
    "    return result_line\n",
    "\n",
    "def vectorize_directory(directory_path, file_name):\n",
    "    all_lines = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            line = vectorize_file(file_path)\n",
    "            all_lines.append(line)\n",
    "            \n",
    "    tsv_filepath = os.path.join('..', 'assets', 'annotated-corpus', file_name + '.tsv')\n",
    "\n",
    "    os.makedirs(os.path.dirname(tsv_filepath), exist_ok=True)\n",
    "\n",
    "    with open(tsv_filepath, 'w') as f:\n",
    "        f.write(\"\\n\".join(all_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../dataset/20news-bydate-test\"\n",
    "\n",
    "vectorize_directory(dataset_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../dataset/20news-bydate-train\"\n",
    "\n",
    "vectorize_directory(dataset_path, 'train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
