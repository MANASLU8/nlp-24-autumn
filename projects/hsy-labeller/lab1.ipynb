{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWdF7ZIsCEZq",
    "outputId": "5a281041-5944-4b7a-9508-3cd9db7a9d5b"
   },
   "source": [
    "!wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n",
    "!tar -xvzf ./20news-bydate.tar.gz"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PmDU84ihFLFQ",
    "ExecuteTime": {
     "end_time": "2024-09-30T18:48:15.170965Z",
     "start_time": "2024-09-30T18:48:14.685066Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "def read_files_from_directories(base_directory):\n",
    "    \"\"\"\n",
    "    Reads and concatenates the content of all files within the specified directory and its subdirectories.\n",
    "    \n",
    "    :param base_directory: The root directory to start reading files from.\n",
    "    :return: A list of strings, each representing the concatenated content of one file.\n",
    "    \"\"\"\n",
    "    all_file_contents = []\n",
    "\n",
    "    # Walk through all directories and files in the base directory\n",
    "    for current_dir, _, files in os.walk(base_directory):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(current_dir, file_name)\n",
    "            \n",
    "            try:\n",
    "                # Open the file and read its content\n",
    "                with open(file_path, 'r', errors='ignore') as file:\n",
    "                    file_content = file.read()\n",
    "                    # Clean the content by replacing newlines and tabs with spaces\n",
    "                    cleaned_content = file_content.replace('\\n', ' ').replace('\\t', ' ')\n",
    "                    all_file_contents.append(cleaned_content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "\n",
    "    return all_file_contents\n",
    "\n",
    "# Define the root directory where the files are located\n",
    "root_directory = '20news-bydate-test'\n",
    "\n",
    "# Read the contents of all files in the root directory\n",
    "all_file_contents = read_files_from_directories(root_directory)\n",
    "\n",
    "# Join all contents into a single string (optional step depending on use case)\n",
    "all_file_contents_joined = ' '.join(all_file_contents)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rBi7e6XlFSVX",
    "outputId": "7ca65d6a-1b89-4510-9d27-02d2e767a482"
   },
   "source": [
    "all_file_contents_joined"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Regular expression pattern to match sentence-ending punctuation followed by whitespace or end of string\n",
    "sentence_end_pattern = r'([.!?])(?=\\s|$)'\n",
    "\n",
    "def split_sentences_with_end_symbols(text):\n",
    "    \"\"\"\n",
    "    Splits the input text into sentences based on sentence-ending punctuation marks (. ! ?).\n",
    "    \n",
    "    :param text: The input text to be split into sentences.\n",
    "    :return: A list of sentences.\n",
    "    \"\"\"\n",
    "    parts = re.split(sentence_end_pattern, text)\n",
    "    sentences = [''.join([parts[i], parts[i + 1]]) for i in range(0, len(parts) - 1, 2)]\n",
    "    return sentences\n",
    "\n",
    "# Assuming all_file_contents_joined is the concatenated content of all files\n",
    "all_file_contents_joined = ' '.join(all_file_contents)\n",
    "\n",
    "# Split the concatenated content into sentences\n",
    "sentences = split_sentences_with_end_symbols(all_file_contents_joined)\n",
    "\n",
    "# Print each sentence\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJkuwbIJX-SZ",
    "outputId": "40fe08b8-c95a-4b03-a6e3-e8fcdc50f0cf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Regular expression patterns with examples in comments\n",
    "\n",
    "# Matches words, emails, numbers, and non-word non-space characters\n",
    "word_pattern = r'\\b[\\w\\'-]+(?:@[\\w.]+)?\\b[^\\w\\s]*'\n",
    "# Examples: \"hello\", \"world's\", \"email@example.com\", \"123\", \"@\", \"#\"\n",
    "\n",
    "# Matches email addresses\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "# Examples: \"user@example.com\", \"john.doe@company.co.uk\", \"test123@test-domain.org\"\n",
    "\n",
    "# Matches Chinese phone numbers\n",
    "phone_china_pattern = r\"^(?:\\+86)(?:(?:-\\d{3}-|\\(\\d{3}\\))\\d{4}-\\d{4}|\\d{11})\"\n",
    "# Examples: \"+86-10-1234-5678\", \"+86(10)1234-5678\", \"+8612345678901\"\n",
    "\n",
    "# Matches Russian phone numbers\n",
    "phone_ru_pattern = r\"^(?:\\+7|8)(?:(?:-\\d{3}-|\\(\\d{3}\\))\\d{3}-\\d{2}-\\d{2}|\\d{10})\"\n",
    "# Examples: \"+7-123-456-78-90\", \"+7(123)456-78-90\", \"81234567890\"\n",
    "\n",
    "# Matches US phone numbers\n",
    "phone_usa_pattern = r\"^(?:\\+1)(?:(?:-\\d{3}-|\\(\\d{3}\\))\\d{3}-\\d{4}|\\d{10})\"\n",
    "# Examples: \"+1-123-456-7890\", \"+1(123)456-7890\", \"+11234567890\"\n",
    "\n",
    "# General phone number pattern\n",
    "phone_pattern = r'(\\+\\d{1,3}\\s?)?(?:\\(\\d{1,3}\\)|\\d{1,3})[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}(?:[-.\\s]?\\d{1,4})?'\n",
    "# Examples: \"+1 123-456-7890\", \"(123)456-7890\", \"123 456 7890\", \"+44 123 456 7890\"\n",
    "\n",
    "# Matches ordinal numbers and possessives\n",
    "numeral_pattern = r\"\\b\\d+(?:th|'s)\\b\"\n",
    "# Examples: \"1st\", \"2nd\", \"3rd\", \"4th\", \"123's\"\n",
    "\n",
    "# Matches dates\n",
    "dates_pattern = r'\\b\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}\\b'\n",
    "# Examples: \"01/01/2023\", \"01-01-2023\", \"01.01.2023\", \"1/1/23\"\n",
    "\n",
    "# Matches times\n",
    "times_pattern = r'\\b(?:[01]?[0-9]|2[0-3]):[0-5][0-9]\\b'\n",
    "# Examples: \"12:34\", \"09:45\", \"1:00\", \"23:59\"\n",
    "\n",
    "# Matches emojis\n",
    "emoji_pattern = r'[:;][-]*[)D\\(\\[\\]PpOo/\\\\]'\n",
    "# Examples: :-), :)\n",
    "\n",
    "# Matches mathematical formulas\n",
    "math_formula_pattern = r'\\b[a-zA-Z]\\s*=\\s*[a-zA-Z0-9\\+\\-\\*/\\(\\)\\[\\]\\^,\\s]+\\s*;?\\b'\n",
    "# Example: a + b = c\n",
    "\n",
    "# Compile regular expressions for efficiency\n",
    "word_re = re.compile(word_pattern)\n",
    "email_re = re.compile(email_pattern)\n",
    "phone_re = re.compile(f\"({phone_pattern})|({phone_ru_pattern})|({phone_usa_pattern})|({phone_china_pattern})\")\n",
    "numeral_re = re.compile(numeral_pattern)\n",
    "dates_re = re.compile(dates_pattern)\n",
    "times_re = re.compile(times_pattern)\n",
    "emoji_re = re.compile(emoji_pattern)\n",
    "math_formula_re = re.compile(math_formula_pattern)\n",
    "\n",
    "pattern_map = {\n",
    "    'math': math_formula_re,\n",
    "    'email': email_re,\n",
    "    'phone': phone_re,\n",
    "    'numeral': numeral_re,\n",
    "    'date': dates_re,\n",
    "    'time': times_re,\n",
    "    'emoji': emoji_re,\n",
    "    'word': word_re\n",
    "}\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    index = 0\n",
    "    while index < len(sentence):\n",
    "        for pattern_name, pattern in pattern_map.items():\n",
    "            match = pattern.match(sentence, index)\n",
    "            if match:\n",
    "                token = match.group()\n",
    "                tokens.append(token)\n",
    "                index = match.end()\n",
    "                break\n",
    "        else:\n",
    "            # If no match is found, move to the next character\n",
    "            index += 1\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Hello world! My email is user@example.com. Phone: +1-123-456-7890. Date: 01/01/2023. Time: 12:34. Math: a + b = c.\"\n",
    "tokens = tokenize(sentence)\n",
    "print(tokens)"
   ],
   "metadata": {
    "id": "R3i1nz1D-FUD",
    "ExecuteTime": {
     "end_time": "2024-10-01T06:07:45.077150Z",
     "start_time": "2024-10-01T06:07:45.072466Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert NLTK POS tags to WordNet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun"
   ],
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alwlqwH4eT_k",
    "outputId": "936ca70d-645a-4b21-d8e9-6ad7e0d8ed61"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def get_stub_stem(token):\n",
    "    stem = stemmer.stem(token)\n",
    "    return stem\n",
    "\n",
    "def get_stub_lemma(token, pos_tag=None):\n",
    "    if pos_tag:\n",
    "        lemma = lemmatizer.lemmatize(token, pos=pos_tag)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "    return lemma\n",
    "\n",
    "def create_annotation(sentences):\n",
    "    annotations = []\n",
    "    for sentence_index, sentence in enumerate(sentences):\n",
    "        tokens = tokenize(sentence)\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "        for token_index, (token, tag) in enumerate(tagged_tokens):\n",
    "            pos_tag = get_wordnet_pos(tag)\n",
    "            stem = get_stub_stem(token)\n",
    "            lemma = get_stub_lemma(token, pos_tag)\n",
    "            annotations.append(f\"{sentence_index + 1}_{token_index + 1}\\t{token}\\t{pos_tag}\\t{stem}\\t{lemma}\")\n",
    "        annotations.append(\"\")\n",
    "    return annotations"
   ],
   "metadata": {
    "id": "GT8e9GWleOWu",
    "ExecuteTime": {
     "end_time": "2024-09-30T18:48:26.255703Z",
     "start_time": "2024-09-30T18:48:26.252366Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:49:36.667221Z",
     "start_time": "2024-09-30T18:48:26.261713Z"
    },
    "id": "znFxvGBeCqk3"
   },
   "cell_type": "code",
   "source": [
    "annotations = create_annotation(sentences)\n",
    "\n",
    "with open('annotations_hsy.tsv', 'w') as file:\n",
    "    file.write(\"\\n\".join(annotations))"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "Dqo1JDCZCqk3",
    "outputId": "5be98828-15ba-491d-bd46-7385f3c83ccc"
   },
   "cell_type": "code",
   "source": [
    "# Test homonyms (duck)\n",
    "\n",
    "sentences_homonym = [\n",
    "    \"Children like to feed the ducks in the pond at the park.\",\n",
    "    \"The boy is throwing a rock at my head, but I am ducking so it doesn’t hit me.\"\n",
    "]\n",
    "\n",
    "annotations_homonyms = create_annotation(sentences_homonym)\n",
    "\n",
    "for annotation in annotations_homonyms:\n",
    "    print(annotation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "if 'TestTokenizeFunction' in globals():\n",
    "    del globals()['TestTokenizeFunction']"
   ],
   "metadata": {
    "id": "reBT5drq8orT",
    "ExecuteTime": {
     "end_time": "2024-09-30T18:49:36.689954Z",
     "start_time": "2024-09-30T18:49:36.687807Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "import unittest\n",
    "\n",
    "class TestTokenize(unittest.TestCase):\n",
    "    def test_email(self):\n",
    "        original = \"My email is test@example.com\"\n",
    "        expected = ['My', 'email', 'is', 'test@example.com']\n",
    "        self.assertEqual(tokenize(original), expected)\n",
    "\n",
    "    def test_phone(self):\n",
    "        original = \"My phone is +1 123-456-7890\"\n",
    "        expected = ['My', 'phone', 'is', '+1 123-456-7890']\n",
    "        self.assertEqual(tokenize(original), expected)\n",
    "\n",
    "    def test_date_and_time(self):\n",
    "        original = \"Now the time is 2:13 01/10/2024\"\n",
    "        expected = ['Now', 'the', 'time', 'is', '2:13', '01/10/2024']\n",
    "        self.assertEqual(tokenize(original), expected)\n",
    "\n",
    "    def test_emoji(self):\n",
    "        original = \"Hello :)\"\n",
    "        expected = ['Hello', ':)']\n",
    "        self.assertEqual(tokenize(original), expected)\n",
    "\n",
    "    def test_mix(self):\n",
    "        self.maxDiff = None\n",
    "        original = \"Children like to feed the ducks in the pond at the park. The boy is throwing a rock at my head, but I am ducking so it doesn’t hit me. Email: example@example.com, Phone: +1 123-456-7890, Date: 10/01/2023, Time: 13:45. I'm happy :) and you? a = b*c;\"\n",
    "        expected = ['Children', 'like', 'to', 'feed', 'the', 'ducks', 'in', 'the', 'pond', 'at', 'the', 'park.', 'The', 'boy', 'is', 'throwing', 'a', 'rock', 'at', 'my', 'head,', 'but', 'I', 'am', 'ducking', 'so', 'it', 'doesn’', 't', 'hit', 'me.', 'Email:', 'example@example.com', 'Phone:', '+1 123-456-7890', 'Date:', '10/01/2023', 'Time:', '13:45', \"I'm\", 'happy', ':)', 'and', 'you?', 'a = b*c']\n",
    "        self.assertEqual(tokenize(original), expected)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ],
   "metadata": {
    "id": "DX5OaorA43Ta",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "62daa6c9-9600-4512-aff0-8229095c22b3"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
