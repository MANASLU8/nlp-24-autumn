{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-uDm_84sHAYs",
    "outputId": "cc77ec8b-d892-458e-9210-37c51e6ac5c2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sentences = list()\n",
    "sentence = list()\n",
    "with open('annotations_hsy2.tsv') as file:\n",
    "  for line in file:\n",
    "      if line != \"\\n\":\n",
    "        sentence.append(line.split('\\t')[1])\n",
    "      if line == \"\\n\" and sentence:\n",
    "        sentences.append(sentence)\n",
    "        sentence = list()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clear_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZyT1Cu8MRB5",
    "outputId": "60d54b9e-06b0-43ec-c8ed-e75f573fdfc4"
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    cleaned = []\n",
    "    for lemma in sentence:\n",
    "        cleaned_tek = re.sub(r\"[^\\w\\s]|[\\d]\", \"\", lemma.lower())\n",
    "        if len(cleaned_tek) != len(lemma) or len(cleaned_tek) <3:\n",
    "          continue;\n",
    "\n",
    "        if cleaned_tek and cleaned_tek not in stop_words:\n",
    "            cleaned.append(cleaned_tek)\n",
    "    return cleaned\n",
    "\n",
    "for sentence in sentences:\n",
    "    clear_data.append(clean_sentence(sentence))\n",
    "print(clear_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlN9B6FnHgD3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uwf_6gtIdWM5",
    "ExecuteTime": {
     "end_time": "2024-10-20T15:15:09.231825900Z",
     "start_time": "2024-10-20T15:14:53.334439700Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(sentences=clear_data, epochs=70, window=4, min_count=3)\n",
    "w2v.wv.most_similar(\"cpu\")\n",
    "\n",
    "words = list(w2v.wv.key_to_index.keys())\n",
    "print(\"All words in model:\", words)\n",
    "\n",
    "word_vector = w2v.wv['astronomers'] \n",
    "print(\"Victor of word 'astronomers':\", word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. С использованием библиотечной реализации метода подсчета косинусного расстояния между векторными представлениями текста"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7iT9-c_A3Gu",
    "outputId": "fdfb922c-d926-41e1-f9df-89b605b9e580"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def custom_cosine_distance(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    Calculate the cosine distance between two vectors (ranging from 0 to 1).\n",
    "    The formula is: (1 - dot_product(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))) / 2\n",
    "    \"\"\"\n",
    "    return (1 - np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))) / 2\n",
    "\n",
    "# Assume w2v is a pre-trained word vector model\n",
    "w2v = None  # This should be replaced with the actual word vector model\n",
    "\n",
    "target_word = \"cpu\"\n",
    "similar_terms = [\"memory\", \"hardware\", \"disk\"]\n",
    "related_terms = [\"software\", \"coding\", \"keyboard\"] \n",
    "unrelated_terms = [\"philosophy\", \"hypothesis\", \"president\"]\n",
    "\n",
    "def print_distances(target, terms, description):\n",
    "    \"\"\"\n",
    "    Print the cosine distances between the target word and a list of terms.\n",
    "    \n",
    "    :param target: The target word\n",
    "    :param terms: A list of terms to compare with the target word\n",
    "    :param description: A description of the type of terms being compared\n",
    "    \"\"\"\n",
    "    print(f\"Cosine distances ({description}):\")\n",
    "    for term in terms:\n",
    "        distance = custom_cosine_distance(w2v.wv[target], w2v.wv[term])\n",
    "        print(f\"{target} - {term}: {distance:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Print cosine distances for different categories of terms\n",
    "print_distances(target_word, similar_terms, \"Similar Terms\")\n",
    "print_distances(target_word, related_terms, \"Related Terms\")\n",
    "print_distances(target_word, unrelated_terms, \"Unrelated Terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Применить какой-либо метод сокращения размерностей полученных одним из базовых способов векторизации"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "AVxQBoXMjyxS",
    "outputId": "9331e08a-1618-4817-cc65-b63b11a570c2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_vectors(words, vector_data):\n",
    "    \"\"\"\n",
    "    Plot the 2D representation of word vectors using PCA.\n",
    "\n",
    "    :param words: List of words to plot\n",
    "    :param vector_data: Dictionary or Word2Vec model containing word vectors\n",
    "    \"\"\"\n",
    "    # Perform PCA to reduce the dimensionality to 2D\n",
    "    pca_model = PCA(n_components=2)\n",
    "    reduced_vectors = pca_model.fit_transform([vector_data[word] for word in words])\n",
    "\n",
    "    # Convert the reduced vectors to a DataFrame\n",
    "    reduced_df = pd.DataFrame(reduced_vectors, index=words, columns=[\"x_coord\", \"y_coord\"])\n",
    "\n",
    "    # Create a scatter plot using Seaborn\n",
    "    scatter_plot = sns.scatterplot(data=reduced_df, x=\"x_coord\", y=\"y_coord\")\n",
    "\n",
    "    # Add labels for each point\n",
    "    for word, coordinates in reduced_df.iterrows():\n",
    "        scatter_plot.text(coordinates[\"x_coord\"], coordinates[\"y_coord\"], word, fontsize=9, ha='right')\n",
    "\n",
    "    # Set the title and axis labels\n",
    "    plt.title(\"2D Representation of Word Vectors using PCA\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example list of words to plot\n",
    "words_to_plot = [\"black\", \"white\", \"color\", \"philosophy\", \"religion\", \"computer\", \"pig\", \"dog\"]\n",
    "\n",
    "# Assume w2v is a pre-trained word vector model\n",
    "w2v = None  # This should be replaced with the actual word vector model\n",
    "\n",
    "# Call the function to plot the word vectors\n",
    "plot_word_vectors(words_to_plot, w2v.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym70N2Moc-0d"
   },
   "source": [
    "6. Implement a method that vectorizes arbitrary text using the following algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fbKZp52uEaM",
    "outputId": "fd503f04-5baa-49e2-bb56-32024f6c42fd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_vectors(sentences, w2v):\n",
    "    final_vector = np.zeros(w2v.vector_size)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_sentence_vector = np.zeros(w2v.vector_size)\n",
    "\n",
    "        for word in sentence:\n",
    "            if word in w2v.wv.key_to_index:\n",
    "                current_sentence_vector += w2v.wv[word]\n",
    "\n",
    "        if len(sentence) > 0:\n",
    "            current_sentence_vector /= len(sentence)\n",
    "\n",
    "        final_vector += current_sentence_vector\n",
    "\n",
    "    if len(sentences) > 0:\n",
    "        final_vector /= len(sentences)\n",
    "\n",
    "    return final_vector\n",
    "\n",
    "calculate_vectors(clear_data, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ZuetI63iSTUX",
    "ExecuteTime": {
     "end_time": "2024-10-20T19:01:54.643313800Z",
     "start_time": "2024-10-20T19:01:44.548393400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import chardet\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "def detect_file_encoding(file_path):\n",
    "    \"\"\"\n",
    "    Detect the encoding of a given file.\n",
    "\n",
    "    :param file_path: Path to the file\n",
    "    :return: Detected encoding\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(10000000))\n",
    "    return result['encoding']\n",
    "\n",
    "def process_text_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a text file to extract sentences.\n",
    "\n",
    "    :param file_path: Path to the file\n",
    "    :return: List of processed sentences\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    with open(file_path, 'r', encoding=detect_file_encoding(file_path)) as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Check if the line is not empty\n",
    "                current_sentence.append(line.split('\\t')[1].strip())\n",
    "            elif current_sentence:  # If the line is empty and we have a sentence\n",
    "                sentences.append(current_sentence)\n",
    "                current_sentence = []\n",
    "\n",
    "    clean_sentences = [clean_sentence(sent) for sent in sentences]\n",
    "    return clean_sentences\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Clean a single sentence by removing unnecessary characters or tokens.\n",
    "\n",
    "    :param sentence: List of tokens in a sentence\n",
    "    :return: Cleaned sentence\n",
    "    \"\"\"\n",
    "    # Placeholder for actual cleaning logic\n",
    "    return sentence\n",
    "\n",
    "def save_vectors_to_tsv(vectors, output_path):\n",
    "    \"\"\"\n",
    "    Save document vectors to a TSV file.\n",
    "\n",
    "    :param vectors: Dictionary of document IDs and their corresponding vectors\n",
    "    :param output_path: Path to the output TSV file\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for doc_id, vector in vectors.items():\n",
    "            vector_str = '\\t'.join(map(str, vector))\n",
    "            f.write(f\"{doc_id}\\t{vector_str}\\n\")\n",
    "\n",
    "def calculate_document_vectors(sentences, word_vectors):\n",
    "    \"\"\"\n",
    "    Calculate the document vector based on the average of word vectors.\n",
    "\n",
    "    :param sentences: List of processed sentences\n",
    "    :param word_vectors: Word2Vec model or dictionary of word vectors\n",
    "    :return: Document vector\n",
    "    \"\"\"\n",
    "    # Placeholder for actual vector calculation logic\n",
    "    document_vector = np.mean([word_vectors[word] for sentence in sentences for word in sentence if word in word_vectors], axis=0)\n",
    "    return document_vector\n",
    "\n",
    "def process_directory(directory_path, output_path):\n",
    "    \"\"\"\n",
    "    Process all TSV files in a directory and save the document vectors to a TSV file.\n",
    "\n",
    "    :param directory_path: Path to the directory containing TSV files\n",
    "    :param output_path: Path to the output TSV file\n",
    "    :return: Dictionary of document IDs and their corresponding vectors\n",
    "    \"\"\"\n",
    "    document_vectors = {}\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".tsv\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            doc_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            processed_sentences = process_text_file(file_path)\n",
    "            document_vector = calculate_document_vectors(processed_sentences, w2v)\n",
    "            document_vectors[doc_id] = document_vector\n",
    "\n",
    "    save_vectors_to_tsv(document_vectors, output_path)\n",
    "    return document_vectors\n",
    "\n",
    "# Example usage\n",
    "directory_path = 'corpus'\n",
    "output_tsv_path = 'space_vec.tsv'\n",
    "w2v = None  # This should be replaced with the actual Word2Vec model\n",
    "\n",
    "document_vectors = process_directory(directory_path, output_tsv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VKMAvge7QeYH",
    "ExecuteTime": {
     "end_time": "2024-10-20T14:40:45.928318900Z",
     "start_time": "2024-10-20T14:40:45.884111300Z"
    }
   },
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in clear_data for item in sublist]\n",
    "unique_words = list(set(flat_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ijwX-GTZ6nV"
   },
   "source": [
    "1 задание - сохранение матрицы термин документ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gq8nnxPSTUHG",
    "ExecuteTime": {
     "end_time": "2024-10-20T14:40:53.896814800Z",
     "start_time": "2024-10-20T14:40:45.941846700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Process all TSV files in a directory and collect tokenized documents.\n",
    "\n",
    "    :param directory_path: Path to the directory containing TSV files\n",
    "    :return: Dictionary of document IDs and their corresponding token lists\n",
    "    \"\"\"\n",
    "    document_texts = {}\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".tsv\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            doc_id = os.path.splitext(filename)[0]  # Remove file extension\n",
    "            document_texts[doc_id] = [item for sublist in process_file(file_path) for item in sublist]\n",
    "    return document_texts\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a single TSV file to extract and tokenize sentences.\n",
    "\n",
    "    :param file_path: Path to the TSV file\n",
    "    :return: List of token lists\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Check if the line is not empty\n",
    "                current_sentence.append(line.split('\\t')[1].strip())\n",
    "            elif current_sentence:  # If the line is empty and we have a sentence\n",
    "                sentences.append(current_sentence)\n",
    "                current_sentence = []\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_term_document_matrix(doc_texts):\n",
    "    \"\"\"\n",
    "    Create a term-document matrix from the tokenized documents.\n",
    "\n",
    "    :param doc_texts: Dictionary of document IDs and their corresponding token lists\n",
    "    :return: Term-document matrix, document index, token index, and token frequency\n",
    "    \"\"\"\n",
    "    token_frequency = defaultdict(int)\n",
    "    for doc_id, tokens in doc_texts.items():\n",
    "        for token in tokens:\n",
    "            token_frequency[token] += 1\n",
    "    unique_tokens = list(token_frequency.keys())\n",
    "\n",
    "    token_index = {token: i for i, token in enumerate(unique_tokens)}\n",
    "\n",
    "    # Create a matrix to store data\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "\n",
    "    doc_index = {doc_id: idx for idx, doc_id in enumerate(doc_texts.keys())}\n",
    "\n",
    "    for doc_id, tokens in doc_texts.items():\n",
    "        doc_idx = doc_index[doc_id]\n",
    "        token_counts = Counter(tokens)\n",
    "        for token, count in token_counts.items():\n",
    "            if token in token_index:\n",
    "                token_idx = token_index[token]\n",
    "                rows.append(doc_idx)\n",
    "                cols.append(token_idx)\n",
    "                data.append(count)\n",
    "\n",
    "    term_doc_matrix = csr_matrix((data, (rows, cols)), shape=(len(doc_texts), len(unique_tokens)))\n",
    "\n",
    "    return term_doc_matrix, doc_index, token_index, token_frequency\n",
    "\n",
    "def save_term_document_matrix_to_tsv(term_doc_matrix, doc_index, token_index, output_tsv_path, unique_tokens):\n",
    "    \"\"\"\n",
    "    Save the term-document matrix to a TSV file.\n",
    "\n",
    "    :param term_doc_matrix: Term-document matrix\n",
    "    :param doc_index: Dictionary mapping document IDs to indices\n",
    "    :param token_index: Dictionary mapping tokens to indices\n",
    "    :param output_tsv_path: Path to the output TSV file\n",
    "    :param unique_tokens: List of unique tokens\n",
    "    \"\"\"\n",
    "    dense_matrix = term_doc_matrix.toarray()\n",
    "    with open(output_tsv_path, 'w', encoding='utf-8') as tsv_file:\n",
    "        # Write header\n",
    "        header = ['doc_id'] + unique_tokens\n",
    "        tsv_file.write('\\t'.join(header) + '\\n')\n",
    "\n",
    "        # Write document rows\n",
    "        for doc_id, doc_idx in doc_index.items():\n",
    "            row = [doc_id] + list(map(str, dense_matrix[doc_idx]))\n",
    "            tsv_file.write('\\t'.join(row) + '\\n')\n",
    "\n",
    "        # Write total row\n",
    "        token_sums = dense_matrix.sum(axis=0)\n",
    "        sum_row = ['Total'] + list(map(str, token_sums))\n",
    "        tsv_file.write('\\t'.join(sum_row) + '\\n')\n",
    "\n",
    "# Example usage\n",
    "directory_path = 'corpus'\n",
    "output_tsv_path = 'term_doc_matrix.tsv'\n",
    "\n",
    "document_texts = process_directory(directory_path)\n",
    "term_doc_matrix, doc_index, token_index, token_frequency = create_term_document_matrix(document_texts)\n",
    "unique_tokens = list(token_frequency.keys())\n",
    "save_term_document_matrix_to_tsv(term_doc_matrix, doc_index, token_index, output_tsv_path, unique_tokens)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
