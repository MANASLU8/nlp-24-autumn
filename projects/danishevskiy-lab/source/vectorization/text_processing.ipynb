{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Собираем все токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "filter_out_tokens = ['\\n', '.', ',', '!', '?', '...', ':', ';']\n",
    "stop_words = set(stopwords.words('english'))\n",
    "broken_files = set()\n",
    "\n",
    "def data_to_text(data, column='lemma'):\n",
    "    return ' '.join(data[column].values)\n",
    "\n",
    "def read_data_with_filter(filename):\n",
    "\n",
    "    rows = []\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if line == '\\n' and len(sentence):\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            if line[0] in filter_out_tokens:\n",
    "                continue\n",
    "            try:\n",
    "                token, stem, lemma = line.split('\\t')\n",
    "                # выкидываем ещё и цифры\n",
    "                lemma_ = re.sub(r'_',' ', lemma)\n",
    "                lemma_ = re.sub(r'[^\\w\\s]|[\\d]|ca|re','', lemma_)\n",
    "                token_ = re.sub(r'[^\\w\\d\\s\\d]','', token)\n",
    "                if len(lemma) != len(lemma_):\n",
    "                    continue\n",
    "                lemma = lemma_\n",
    "                if len(lemma) == 0 or len(lemma.strip()) == 0 or token in stop_words:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(filename, line, e)\n",
    "                \n",
    "            rows.append((token.lower(), stem.lower(),lemma.strip().lower()))\n",
    "            sentence.append(lemma.strip().lower())\n",
    "\n",
    "\n",
    "\n",
    "    return pd.DataFrame(rows, columns=('token', 'stem', 'lemma')), sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем все файлы, достаём все слова\n",
    "topics = ['baseball', 'electronics', 'hockey', 'med', 'motorcycles', 'space']\n",
    "data_path = '../../assets\\\\annotated-corpus'\n",
    "\n",
    "corpuses = []\n",
    "corpus_sentences = []\n",
    "\n",
    "for topic in topics:\n",
    "    cur_data_path = os.path.join(data_path, topic)\n",
    "    files = os.listdir(cur_data_path)\n",
    "    filenames = [os.path.join(cur_data_path, fname) for fname in files]\n",
    "    print('number of files: ', len(filenames))\n",
    "    for f in filenames:\n",
    "        corpus, sentences = read_data_with_filter(f)\n",
    "        corpuses.append(corpus)\n",
    "        corpus_sentences.extend(sentences)\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "for c in corpuses:\n",
    "    unique_tokens = pd.unique(c['lemma'])\n",
    "    unique_words.update(unique_tokens.tolist())\n",
    "print('num of unique words: ', len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def get_term_document_matrix(unique_words: set, documents: list[pd.DataFrame]):\n",
    "    print(len(documents))\n",
    "    result = []\n",
    "    i = 0\n",
    "    for word in tqdm.tqdm(unique_words):\n",
    "        summ_freq = 0\n",
    "        term_doc_vector = []\n",
    "        for d in documents:\n",
    "            curr_freq = len(d[d['lemma'] == word])\n",
    "            term_doc_vector.append(curr_freq)\n",
    "            summ_freq += curr_freq\n",
    "        result.append([word, summ_freq, *term_doc_vector])\n",
    "        i += 1\n",
    "    result = pd.DataFrame(result)\n",
    "    result.columns = ['word', 'freq', *[f'd{i}' for i in range(len(documents))]]\n",
    "    result.set_index('word')\n",
    "    return result\n",
    "\n",
    "term_document_matrix_df = get_term_document_matrix(unique_words, corpuses[:100])\n",
    "term_document_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_matrix_df[term_document_matrix_df['freq'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_matrix_df.to_csv('../../assets/term_document_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_term_vector(term_document_matrix_df, doc_name):\n",
    "    doc_term_vector = term_document_matrix_df\n",
    "    doc_term_vector = doc_term_vector[['word', doc_name]]\n",
    "    doc_term_vector = doc_term_vector[doc_term_vector[doc_name] > 0]\n",
    "    return doc_term_vector\n",
    "\n",
    "doc_term_vec = get_document_term_vector(term_document_matrix_df, 'd90')\n",
    "doc_term_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..tokenizer.tokenizer import tokenize_text\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    text = text.replace('>', '').replace('|', '').replace('\\n', ' ').replace('-', '')\n",
    "    return text\n",
    "\n",
    "def annotation_2_sentence_list(doc_annotaition):\n",
    "\n",
    "    rows = []\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    lines = doc_annotaition.split('\\n')\n",
    "    for line in lines:\n",
    "        if line == '\\n' and len(sentence):\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        if line[0] in filter_out_tokens:\n",
    "            continue\n",
    "        try:\n",
    "            token, stem, lemma = line.split('\\t')\n",
    "            # выкидываем ещё и цифры\n",
    "            lemma_ = re.sub(r'_',' ', lemma)\n",
    "            lemma_ = re.sub(r'[^\\w\\s]|[\\d]|ca|re','', lemma_)\n",
    "            token_ = re.sub(r'[^\\w\\d\\s\\d]','', token)\n",
    "            if len(lemma) != len(lemma_):\n",
    "                continue\n",
    "            lemma = lemma_\n",
    "            if len(lemma) == 0 or len(lemma.strip()) == 0 or token in stop_words:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(line, e)\n",
    "            \n",
    "        rows.append((token.lower(), stem.lower(),lemma.strip().lower()))\n",
    "        sentence.append(lemma.strip().lower())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tf_idf(term_document_matrix_df, documents, min_freq=5):\n",
    "    filtered_term_document_matrix = term_document_matrix_df[term_document_matrix_df['freq'] > min_freq]\n",
    "\n",
    "    total_documents = len(documents)\n",
    "\n",
    "    # tf\n",
    "    tf_idf_matrix = []\n",
    "    total_words_in_documents = np.array([len(d) for d in documents])\n",
    "    for i in range(len(filtered_term_document_matrix)):\n",
    "        tf_ = filtered_term_document_matrix.iloc[i, 2:].values / total_words_in_documents\n",
    "        idf_ = np.log(total_documents / len(tf_[tf_ > 0]))\n",
    "        tf_idf_ = tf_ * idf_\n",
    "        tf_idf_matrix.append([filtered_term_document_matrix.iloc[i,0], *tf_idf_])\n",
    "    tf_idf_matrix = pd.DataFrame(tf_idf_matrix)\n",
    "    tf_idf_matrix.columns = ['word', *[f'tfidf_score{i}' for i in range(len(documents))]]\n",
    "    \n",
    "    # как использовать tf-ifd как весовой коэф??\n",
    "    mean_tf_idf_matrix = np.mean(tf_idf_matrix.iloc[:, 1:].values, axis=1)\n",
    "    tf_idf_matrix['mean_tf_idf_score'] = mean_tf_idf_matrix\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = tf_idf(term_document_matrix_df, corpuses[:100])\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix.to_csv('../../assets/tf_idf_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "EPOCHS = 40\n",
    "LOWEST_WORD_FREQUENCY=3\n",
    "VECTOR_SIZE=128\n",
    "WINDOW_SIZE = 5\n",
    "WORKERS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('hockey_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    corpus_sentences,\n",
    "    min_count=LOWEST_WORD_FREQUENCY, \n",
    "    epochs=40,\n",
    "    window=WINDOW_SIZE,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    workers=WORKERS,\n",
    ")\n",
    "\n",
    "model.save('../../assets/big_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.key_to_index.keys())\n",
    "len(words), words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_wordfreq_2_wordnumber():\n",
    "    result = []\n",
    "    for word_freq in [1,2,4,8,16]:\n",
    "        model_ = Word2Vec(\n",
    "            corpus_sentences,\n",
    "            min_count=word_freq, \n",
    "            epochs=40,\n",
    "            window=WINDOW_SIZE,\n",
    "            vector_size=VECTOR_SIZE,\n",
    "            workers=WORKERS,\n",
    "        )\n",
    "        words = list(model_.wv.key_to_index.keys())\n",
    "        result.append([word_freq, len(words)])\n",
    "    p = pd.DataFrame(result, columns=['word freq', 'unique words'])\n",
    "    p.set_index('word freq', inplace=True)\n",
    "    return p\n",
    "\n",
    "inspect_wordfreq_2_wordnumber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.key_to_index.keys())\n",
    "words\n",
    "player_vector = model.wv.get_vector('player')\n",
    "hockey_vector = model.wv.get_vector('hockey')\n",
    "article_vector = model.wv.get_vector('article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return (1.0 - (dot_product / (norm_a * norm_b))) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = cosine_distance(player_vector, hockey_vector)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = cosine_distance(player_vector, article_vector)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "hockey_group = [\n",
    "    \"player\", \"goal\", \"playoff\"\n",
    "]\n",
    "\n",
    "education_research_group = [\n",
    "    \"university\", \"research\", \"department\", \"computer\", \"science\",\n",
    "]\n",
    "\n",
    "media_communication_group = [\n",
    "   \"news\", \"article\", \"post\", \"email\", \"comment\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_similariies(words, model: Word2Vec):\n",
    "    n_words = len(words)\n",
    "    result_matrix = np.empty((0, n_words))\n",
    "    for w_1 in words:\n",
    "        w1_vector = model.wv.get_vector(w_1)\n",
    "        word_sims = []\n",
    "        for w_2 in words:\n",
    "            w2_vector = model.wv.get_vector(w_2)\n",
    "            sim = cosine_distance(w1_vector, w2_vector)\n",
    "            word_sims.append(sim)\n",
    "        result_matrix = np.vstack((result_matrix, word_sims),dtype=np.float16)\n",
    "    print(result_matrix.shape)\n",
    "    df = pd.DataFrame(result_matrix, columns=words)\n",
    "    df.index = words\n",
    "    return df\n",
    "\n",
    "get_all_similariies([*hockey_group, *education_research_group, *media_communication_group], model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similar_groups(words: list[str], model: Word2Vec):\n",
    "    from sklearn.decomposition import PCA\n",
    "    import matplotlib.pyplot as plt\n",
    "    result = []\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        word_vector = model.wv[word]\n",
    "        word_vectors.append(word_vector)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(word_vectors)\n",
    "    for word in words:\n",
    "        result.append([word, *pca.transform([model.wv[word]])[0]])\n",
    "\n",
    "    result = pd.DataFrame(result)\n",
    "    result.columns = ['word', 'x', 'y']\n",
    "    result.set_index('word', inplace=True)\n",
    "    plt.scatter(result['x'].values, result['y'].values)\n",
    "    for i, txt in enumerate(result.index):\n",
    "        plt.annotate(txt, (result['x'].values[i], result['y'].values[i]))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_similar_groups([*hockey_group, *education_research_group, *media_communication_group], model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = pd.read_csv('../../assets/tf_idf_matrix.csv', usecols=['word', 'mean_tf_idf_score'])\n",
    "\n",
    "def vectorize_document(doc, model):\n",
    "    document_vector = np.zeros(VECTOR_SIZE)\n",
    "    for sentence in doc:\n",
    "        sentence_vector = vectorize_sentence(sentence, model)\n",
    "        document_vector += sentence_vector\n",
    "    document_vector /= len(doc)\n",
    "    return document_vector\n",
    "\n",
    "def vectorize_sentence(sentence, model: Word2Vec):\n",
    "    sentence_vector = np.zeros(VECTOR_SIZE)\n",
    "    for w in sentence:\n",
    "        if model.wv.has_index_for(w):\n",
    "            sentence_vector += model.wv.get_vector(w)\n",
    "    sentence_vector /= len(sentence)\n",
    "    return sentence_vector\n",
    "\n",
    "def get_word_weight(word):\n",
    "    weight = 1\n",
    "    try:\n",
    "        tf_idf = tfidf_matrix[tfidf_matrix['word'] == word]['mean_tf_idf_score'].values\n",
    "        if len(tf_idf) and tf_idf > 0:\n",
    "            weight = 1 / tf_idf\n",
    "    except:\n",
    "        pass\n",
    "    if weight > 0:\n",
    "        return weight\n",
    "    return 1\n",
    "\n",
    "def vectorize_sentence_weighted(sentence, model: Word2Vec):\n",
    "    sentence_vector = np.zeros(VECTOR_SIZE)\n",
    "    for w in sentence:\n",
    "        if model.wv.has_index_for(w):\n",
    "            weighter_word_vector = model.wv.get_vector(w) * get_word_weight(w)\n",
    "            sentence_vector += weighter_word_vector\n",
    "    sentence_vector /= len(sentence)\n",
    "    return sentence_vector\n",
    "\n",
    "def get_document_annotation(doc_vector):\n",
    "    return '\\t'.join(list(map( str, doc_vector.tolist())))\n",
    "\n",
    "def save_documents_annotation(path, doc_annotation):\n",
    "    with open(path, 'w') as f:\n",
    "        f.writelines(doc_annotation)\n",
    "\n",
    "def vectorize_documents(filenames, model, annotation_path):\n",
    "    documents_annotations = ''\n",
    "    for i, fname in enumerate(filenames):\n",
    "        _, doc = read_data_with_filter(fname)\n",
    "        doc_vector = vectorize_document(doc, model)\n",
    "        documents_annotations += f'{i+1:04}\\t'\n",
    "        documents_annotations += get_document_annotation(doc_vector) + '\\n'\n",
    "\n",
    "    save_documents_annotation(annotation_path, documents_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = '..\\\\..\\\\assets\\\\annotated-corpus\\\\hockey_train'\n",
    "annot_path = '..\\\\..\\\\assets\\\\hockey_annotations_weighted_.txt'\n",
    "\n",
    "filenames = [os.path.join(data_path, fname) for fname in os.listdir(data_path)]\n",
    "vectorize_documents(filenames, model, annot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '..\\\\..\\\\assets\\\\annotated-corpus\\\\'\n",
    "annot_path = '..\\\\..\\\\assets\\\\vectorized\\\\'\n",
    "\n",
    "model = Word2Vec.load('..\\\\..\\\\assets\\\\big_model')\n",
    "\n",
    "for topic in topics[1:]:\n",
    "    cur_data_path = os.path.join(data_path, topic)\n",
    "    cur_annot_path = os.path.join(annot_path, topic) + '.tsv'\n",
    "    files = os.listdir(cur_data_path)\n",
    "    filenames = [os.path.join(cur_data_path, fname) for fname in files]\n",
    "    print(f'number of files in topic `{topic}`: ', len(filenames))\n",
    "    vectorize_documents(filenames, model, cur_annot_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_virt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
